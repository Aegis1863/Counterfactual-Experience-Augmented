{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tools.utils import train_PPO_agent, compute_advantage, read_ckp, CVAE, cvae_train\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PPO 任务')\n",
    "parser.add_argument('--model_name', default=\"VAE_PPO\", type=str, help='模型名称')\n",
    "parser.add_argument('-w', '--writer', default=1, type=int, help='存档等级, 0: 不存，1: 本地 2: 本地 + wandb本地, 3. 本地 + wandb云存档')\n",
    "parser.add_argument('-o', '--online', action=\"store_true\", help='是否上传wandb云')\n",
    "parser.add_argument('--begin_seed', default=42, type=int, help='起始种子')\n",
    "parser.add_argument('--end_seed', default=42, type=int, help='结束种子')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.writer == 2:\n",
    "    if os.path.exists(\"api_key.txt\"):\n",
    "        with open(\"api_key.txt\", \"r\") as f:  # 该文件中写入一行wandb的API\n",
    "            api_key = f.read()\n",
    "    os.environ[\"WANDB_API_KEY\"] = api_key\n",
    "    os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.h_1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.h_1(F.relu(self.fc1(x))))\n",
    "        return F.softmax(self.fc2(x), dim=-1)\n",
    "    \n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.h_1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.h_1(F.relu(self.fc1(x))))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class VAE_PPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        hidden_dim: int,\n",
    "        action_dim: int,\n",
    "        cave: object,\n",
    "        actor_lr: float=1e-4,\n",
    "        critic_lr: float=5e-3,\n",
    "        gamma: float=0.9,\n",
    "        lmbda: float=0.9,\n",
    "        epochs: int=20,\n",
    "        eps: float=0.2,\n",
    "        device: str='cpu',\n",
    "    ):\n",
    "        \n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma  # 时序差分学习率\n",
    "        self.lmbda = lmbda\n",
    "        self.epochs = epochs  # 一条序列的数据用来训练轮数\n",
    "        self.eps = eps  # PPO中截断范围的参数\n",
    "        self.device = device\n",
    "        self.cvae = cave.to(device)\n",
    "        self.cvae_optimizer = torch.optim.Adam(self.cvae.parameters(), lr=1e-3)\n",
    "\n",
    "    def take_action(self, state) -> list:\n",
    "        state = torch.tensor(state[np.newaxis, :], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(np.array(transition_dict['states']), dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(np.array(transition_dict['actions']), dtype=torch.int64).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(np.array(transition_dict['rewards']), dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(np.array(transition_dict['next_states']), dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(np.array(transition_dict['dones']), dtype=torch.int).view(-1, 1).to(self.device)\n",
    "        truncated = torch.tensor(np.array(transition_dict['truncated']), dtype=torch.int).view(-1, 1).to(self.device)\n",
    "        \n",
    "        # * 技巧\n",
    "        self.train_cvae(states, next_states)  # 训练 vae\n",
    "        pre_next_state = self.predict_next_state(states, next_states)\n",
    "        target_q1 = self.critic(pre_next_state).detach()\n",
    "        target_q2 = self.critic(next_states).detach()\n",
    "        target_q = torch.min(target_q1, target_q2)\n",
    "        \n",
    "        td_target = rewards + self.gamma * target_q * (1 - dones | truncated)\n",
    "        td_delta = td_target - self.critic(states)\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)\n",
    "        # 所谓的另一个演员就是原来的演员的初始状态\n",
    "        old_log_probs = torch.log(self.actor(states).gather(1, actions)).detach()\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "            ratio = torch.exp(log_probs - old_log_probs)  # 重要性采样系数\n",
    "            surr1 = ratio * advantage  # 重要性采样\n",
    "            surr2 = torch.clip(ratio, 1 - self.eps, 1 + self.eps) * advantage\n",
    "            actor_loss = torch.mean(-torch.min(surr1, surr2))\n",
    "            critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "    def train_cvae(self, state, next_state):\n",
    "        vae_action = next_state[:, :4]\n",
    "        diff_state = next_state[:, 5:] - state[:, 5:]\n",
    "        train_loss = cvae_train(self.cvae, diff_state, vae_action, self.cvae_optimizer)\n",
    "        return train_loss\n",
    "    \n",
    "    def predict_next_state(self, state, next_state):\n",
    "        action = state[:, :4]\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(state.shape[0], 32).to(device)  # 随机采样的\n",
    "            generated = self.cvae.decode(sample, action)\n",
    "        pre_next_state = torch.concat([next_state[:, :5], state[:, 5:] + generated], dim=-1)\n",
    "        return pre_next_state\n",
    "    \n",
    "    \n",
    "# * --------------------- 参数 -------------------------\n",
    "if __name__ == '__main__':\n",
    "    # 环境相关\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    env = gym.make(\n",
    "        \"LunarLander-v2\",\n",
    "        continuous=False,\n",
    "        gravity=-10.0,\n",
    "        enable_wind=False,\n",
    "        wind_power=15.0,\n",
    "        turbulence_power=1.5,\n",
    "    )\n",
    "\n",
    "    # PPO相关\n",
    "    actor_lr = 1e-3\n",
    "    critic_lr = 1e-2\n",
    "    lmbda = 0.95  # 似乎可以去掉，这一项仅用于调整计算优势advantage时，额外调整折算奖励的系数\n",
    "    gamma = 0.98  # 时序差分学习率，也作为折算奖励的系数之一\n",
    "    total_epochs = 1  # 迭代轮数\n",
    "    eps = 0.2  # 截断范围参数, 1-eps ~ 1+eps\n",
    "    epochs = 10  # PPO中一条序列训练多少轮，和迭代算法无关\n",
    "\n",
    "    # 神经网络相关\n",
    "    hidden_dim = 128\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # VAE\n",
    "    cvae = CVAE(32, action_dim, 32)\n",
    "    \n",
    "    # 任务相关\n",
    "    system_type = sys.platform  # 操作系统\n",
    "    print('device:', device)\n",
    "\n",
    "    # * ----------------------- 训练 ----------------------------\n",
    "    for seed in range(args.begin_seed, args.end_seed + 1):\n",
    "        CKP_PATH = f'ckpt/{args.model_name}/{args.net.split(\"/\")[-1].split(\".\")[0]}_{seed}_{system_type}.pt'\n",
    "        env = gym.make(\n",
    "            \"LunarLander-v2\",\n",
    "            continuous=True,\n",
    "            gravity=-10.0,\n",
    "            enable_wind=False,\n",
    "            wind_power=15.0,\n",
    "            turbulence_power=1.5,\n",
    "        )\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        agent = VAE_PPO(state_dim, hidden_dim, action_dim, cvae, actor_lr, \n",
    "                    critic_lr, gamma, lmbda, epochs, eps, device)\n",
    "        (s_epoch, s_episode, return_list,  waitt_list, \n",
    "        queue_list, speed_list, time_list, seed_list) = read_ckp(CKP_PATH, agent, 'PPO')\n",
    "\n",
    "        if args.writer > 1:\n",
    "            wandb.init(\n",
    "                project=\"MBPO-SUMO\",\n",
    "                group=args.model_name,\n",
    "                name=f\"{seed}\",\n",
    "                config={\n",
    "                \"episodes\": args.episodes,\n",
    "                \"seed\": seed,\n",
    "                \"road net\": args.net,\n",
    "                \"mission name\": args.model_name\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return_list, train_time = train_PPO_agent(env, agent, args.writer, s_epoch, total_epochs, \n",
    "                                            s_episode, args.episodes, return_list, queue_list, \n",
    "                                            waitt_list, speed_list, time_list, seed_list, seed, CKP_PATH,\n",
    "                                            )\n",
    "\n",
    "        # * ----------------- 绘图 ---------------------\n",
    "\n",
    "        sns.lineplot(return_list, label=f'{seed}')\n",
    "        plt.title(f'{args.model_name}, training time: {train_time} min')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Return')\n",
    "        plt.savefig(f'image/train_{args.model_name}_{seed}_{system_type}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
