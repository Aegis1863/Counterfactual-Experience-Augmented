{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "\u001b[34m[ checkpoint ]\u001b[0m 读取已有模型权重和训练数据...\n",
      "29.494\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "highway\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utils.highway_utils import train_PPO_agent, compute_advantage, read_ckp\n",
    "from utils.cvae import CVAE, cvae_train\n",
    "# from dynamic_model.train_Ensemble_dynamic_model import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.h_1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.h_1(F.relu(self.fc1(x))))\n",
    "        return F.softmax(self.fc2(x), dim=-1)\n",
    "    \n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.h_1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.h_1(F.relu(self.fc1(x))))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        hidden_dim: int,\n",
    "        action_dim: int,\n",
    "        cvae: object=None,\n",
    "        actor_lr: float=1e-4,\n",
    "        critic_lr: float=5e-3,\n",
    "        gamma: float=0.9,\n",
    "        lmbda: float=0.9,\n",
    "        epochs: int=20,\n",
    "        eps: float=0.2,\n",
    "        device: str='cpu',\n",
    "    ):\n",
    "        \n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma  # 时序差分学习率\n",
    "        self.lmbda = lmbda\n",
    "        self.epochs = epochs  # 一条序列的数据用来训练轮数\n",
    "        self.eps = eps  # PPO中截断范围的参数\n",
    "        self.device = device\n",
    "        if cvae:\n",
    "            self.cvae = cvae.to(device)\n",
    "            self.cvae_optimizer = torch.optim.Adam(self.cvae.parameters(), lr=1e-3)\n",
    "        else:\n",
    "            self.cvae = None\n",
    "\n",
    "    def take_action(self, state) -> list:\n",
    "        state = torch.tensor(state[np.newaxis, :], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(np.array(transition_dict['states']), dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(np.array(transition_dict['actions']), dtype=torch.int64).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(np.array(transition_dict['rewards']), dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(np.array(transition_dict['next_states']), dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(np.array(transition_dict['dones']), dtype=torch.int).view(-1, 1).to(self.device)\n",
    "        truncated = torch.tensor(np.array(transition_dict['truncated']), dtype=torch.int).view(-1, 1).to(self.device)\n",
    "        \n",
    "        # * 技巧\n",
    "        # self.train_cvae(states, next_states)  # 训练 vae, 如果是已经预训练好的就无需训练\n",
    "        # self.cvae_generate(32)  # 生成 cvae 图像观察效果\n",
    "        if self.cvae:\n",
    "            pre_next_state = self.predict_next_state(states, next_states)\n",
    "            target_q1 = self.critic(pre_next_state).detach()\n",
    "            target_q2 = self.critic(next_states).detach()\n",
    "            target_q = torch.min(target_q1, target_q2)\n",
    "        else:\n",
    "            target_q = self.critic(next_states).detach()\n",
    "            \n",
    "        td_target = rewards + self.gamma * target_q * (1 - dones | truncated)\n",
    "        td_delta = td_target - self.critic(states)\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)\n",
    "        # 所谓的另一个演员就是原来的演员的初始状态\n",
    "        old_log_probs = torch.log(self.actor(states).gather(1, actions)).detach()\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "            ratio = torch.exp(log_probs - old_log_probs)  # 重要性采样系数\n",
    "            surr1 = ratio * advantage  # 重要性采样\n",
    "            surr2 = torch.clip(ratio, 1 - self.eps, 1 + self.eps) * advantage\n",
    "            actor_loss = torch.mean(-torch.min(surr1, surr2))\n",
    "            critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "            \n",
    "    def train_cvae(self, state, next_state):\n",
    "        vae_action = next_state[:, :4]\n",
    "        diff_state = next_state[:, 5:] - state[:, 5:]\n",
    "        train_loss = cvae_train(self.cvae, diff_state, vae_action, self.cvae_optimizer)\n",
    "        return train_loss\n",
    "    \n",
    "    def predict_next_state(self, state, next_state):\n",
    "        action = state[:, :4]\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(state.shape[0], 32).to(device)  # 随机采样的\n",
    "            generated = self.cvae.decode(sample, action)\n",
    "        pre_next_state = torch.concat([next_state[:, :5], state[:, 5:] + generated], dim=-1)\n",
    "        return pre_next_state\n",
    "    \n",
    "    \n",
    "# * --------------------- 参数 -------------------------\n",
    "# 环境相关\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "env = gym.make('highway-v0', render_mode='human')\n",
    "seed = 42\n",
    "# env = gym.make(\"highway-v0\", render_mode='rgb_array')\n",
    "\n",
    "\n",
    "# PPO相关\n",
    "actor_lr = 5e-4\n",
    "critic_lr = 1e-3\n",
    "lmbda = 0.95  # 似乎可以去掉，这一项仅用于调整计算优势advantage时，额外调整折算奖励的系数\n",
    "gamma = 0.98  # 时序差分学习率，也作为折算奖励的系数之一\n",
    "total_epochs = 1  # 迭代轮数\n",
    "eps = 0.2  # 截断范围参数, 1-eps ~ 1+eps\n",
    "epochs = 10  # PPO中一条序列训练多少轮，和迭代算法无关\n",
    "\n",
    "# 神经网络相关\n",
    "hidden_dim = 64\n",
    "state_dim = torch.multiply(*env.observation_space.shape)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# VAE\n",
    "# cvae = CVAE(32, action_dim, 32)  # 在线训练\n",
    "# 需要预训练\n",
    "# cvae = torch.load(f'model/cvae/{args.cvae_kind}.pt', map_location=device) if args.cvae_kind else None  \n",
    "\n",
    "# 任务相关\n",
    "system_type = sys.platform  # 操作系统\n",
    "print('device:', device)\n",
    "\n",
    "# * ----------------------- 训练 ----------------------------\n",
    "CKP_PATH = f'ckpt/highway/PPO/{seed}/{system_type}.pt'\n",
    "# env = gym.make(\"highway-v0\", render_mode='rgb_array')\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, None, actor_lr, \n",
    "            critic_lr, gamma, lmbda, epochs, eps, device)\n",
    "s_epoch, s_episode, return_list, time_list, seed_list = read_ckp(CKP_PATH, agent, 'PPO')\n",
    "\n",
    "obs, info = env.reset()\n",
    "done = truncated = False\n",
    "total_reward = 0\n",
    "while not (done | truncated):\n",
    "    obs = obs.reshape(-1)\n",
    "    action = agent.take_action(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(round(total_reward, 3))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[ checkpoint ]\u001b[0m 读取已有模型权重和训练数据...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils.highway_utils import read_ckp, train_DQN\n",
    "from utils.cvae import CVAE, cvae_train\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class VAnet(torch.nn.Module):\n",
    "    ''' 只有一层隐藏层的A网络和V网络 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(VAnet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)  # 共享网络部分\n",
    "        self.h_1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_A = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_V = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        A = self.fc_A(F.relu(self.h_1(F.relu(self.fc1(x)))))\n",
    "        V = self.fc_V(F.relu(self.h_1(F.relu(self.fc1(x)))))\n",
    "        Q = V + A - A.mean(-1).view(-1, 1)  # Q值由V值和A值计算得到\n",
    "        return Q\n",
    "    \n",
    "class DQN:\n",
    "    ''' DQN算法,包括Double DQN '''\n",
    "    \n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate,\n",
    "                 gamma, epsilon, update_interval, sta, device,):\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device)\n",
    "        self.target_q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.update_interval = update_interval\n",
    "        self.sta = sta\n",
    "        # self.sta = sta_kind\n",
    "        self.count = 0\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float).to(self.device)\n",
    "            action = self.q_net(state).argmax().item()\n",
    "        return action\n",
    "\n",
    "    def max_q_value(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float).to(self.device)\n",
    "        return self.q_net(state).max().item()\n",
    "    \n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions'], dtype=torch.int64).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.int).view(-1, 1).to(self.device)\n",
    "        truncated = torch.tensor(transition_dict['truncated'], dtype=torch.int).view(-1, 1).to(self.device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)  # Q值\n",
    "        max_action = self.q_net(next_states).max(1)[1].view(-1, 1)\n",
    "        \n",
    "        # * 技巧一\n",
    "        if self.sta and self.sta.quality > 0.3:\n",
    "            pre_next_state = self.predict_next_state(states, next_states)\n",
    "            target_q1 = self.target_q_net(pre_next_state).detach()\n",
    "            target_q2 = self.target_q_net(next_states).detach()\n",
    "            max_next_q_values = torch.min(target_q1, target_q2)\n",
    "        else:\n",
    "            max_next_q_values = self.target_q_net(next_states).gather(1, max_action)\n",
    "            \n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones | truncated)  # TD误差目标\n",
    "        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  # 均方误差损失函数\n",
    "        self.optimizer.zero_grad()  # PyTorch中默认梯度会累积,这里需要显式将梯度置为0\n",
    "        dqn_loss.backward()  # 反向传播更新参数\n",
    "        self.optimizer.step() # 执行Adam梯度下降\n",
    "\n",
    "        if self.count % self.update_interval == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())  # 更新目标网络\n",
    "        self.count += 1\n",
    "    \n",
    "    def train_cvae(self, state, next_state, test_and_feedback, batch_size):\n",
    "        vae_action = next_state[:, :4]\n",
    "        diff_state = next_state[:, 5:] - state[:, 5:]\n",
    "        loss = cvae_train(self.sta, self.device, diff_state, vae_action, self.sta_optimizer, test_and_feedback, batch_size)\n",
    "        return loss\n",
    "    \n",
    "    def predict_next_state(self, state, next_state):\n",
    "        action = state[:, :4]\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(state.shape[0], 32).to(device)  # 随机采样的\n",
    "            generated = self.sta.decode(sample, action)\n",
    "        pre_next_state = torch.concat([next_state[:, :5], state[:, 5:] + generated], dim=-1)\n",
    "        return pre_next_state\n",
    "\n",
    "    \n",
    "# * --------------------- 参数 -------------------------\n",
    "# 环境相关\n",
    "env = gym.make(\"highway-v0\", render_mode='human')\n",
    "env.configure({\n",
    "    \"lanes_count\": 2,\n",
    "    \"vehicles_density\": 2,\n",
    "    \"duration\": 100,\n",
    "    \"collision_reward\": -30,\n",
    "    \"right_lane_reward\": 0,\n",
    "    \"high_speed_reward\": 0.1,\n",
    "})\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "mission = 'highway'\n",
    "model_name = 'DQN~test'\n",
    "system_type = sys.platform  # 操作系统\n",
    "seed = 42\n",
    "\n",
    "# DQN相关\n",
    "total_epoch = 1  # 迭代数, 无需多次迭代\n",
    "gamma = 0.98\n",
    "epsilon = 1  # 刚开始随机动作,更新中线性降低\n",
    "update_interval = 50  # 若干回合更新一次目标网络\n",
    "minimal_size = 500  # 最小经验数\n",
    "batch_size = 128\n",
    "buffer_size = 20000\n",
    "\n",
    "# 神经网络相关\n",
    "lr = 2e-3\n",
    "state_dim = torch.multiply(*env.observation_space.shape)\n",
    "hidden_dim = 256\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# * ----------------------- 训练 ----------------------------\n",
    "CKP_PATH = f'ckpt/highway/DQN~test/{seed}/{system_type}.pt'\n",
    "agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon, update_interval, None, device)\n",
    "s_epoch, s_episode, return_list, time_list, seed_list, replay_buffer = read_ckp(CKP_PATH, agent, model_name, 20000)\n",
    "agent.epsilon = 0\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.819\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"highway-v0\", render_mode='human')\n",
    "env.configure({\n",
    "    \"lanes_count\": 4,\n",
    "    \"vehicles_density\": 1.5,\n",
    "    \"duration\": 100,\n",
    "    \"collision_reward\": -30,\n",
    "    \"right_lane_reward\": 0,\n",
    "    \"high_speed_reward\": 0.1,\n",
    "})\n",
    "\n",
    "obs, info = env.reset()\n",
    "done = truncated = False\n",
    "total_reward = 0\n",
    "while not (done | truncated):\n",
    "    obs = obs.reshape(-1)\n",
    "    action = agent.take_action(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(round(total_reward, 3))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
